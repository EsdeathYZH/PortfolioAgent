# -*- coding: utf-8 -*-
"""
Gemini AIåˆ†æå™¨å®ç°

ä»analyzer.pyè¿ç§»çš„GeminiAnalyzerç±»å®Œæ•´å®ç°
"""

import json
import logging

# å¯¼å…¥ä¾èµ–
import sys
import threading
import time
from collections import deque
from pathlib import Path
from typing import Any, Dict, List, Optional

from tenacity import before_sleep_log, retry, retry_if_exception_type, stop_after_attempt, wait_exponential

project_root = Path(__file__).parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from common.config import get_config
from core.domain.analysis import AnalysisResult

from .parsers.dashboard_parser import DashboardParser
from .prompts.gold_analysis import GOLD_SYSTEM_PROMPT
from .prompts.stock_analysis import SYSTEM_PROMPT

# è‚¡ç¥¨åç§°æ˜ å°„ï¼ˆå¸¸è§è‚¡ç¥¨ï¼‰
STOCK_NAME_MAP = {
    "600519": "è´µå·èŒ…å°",
    "000001": "å¹³å®‰é“¶è¡Œ",
    "300750": "å®å¾·æ—¶ä»£",
    "002594": "æ¯”äºšè¿ª",
    "600036": "æ‹›å•†é“¶è¡Œ",
    "601318": "ä¸­å›½å¹³å®‰",
    "000858": "äº”ç²®æ¶²",
    "600276": "æ’ç‘åŒ»è¯",
    "601012": "éš†åŸºç»¿èƒ½",
    "002475": "ç«‹è®¯ç²¾å¯†",
    "300059": "ä¸œæ–¹è´¢å¯Œ",
    "002415": "æµ·åº·å¨è§†",
    "600900": "é•¿æ±Ÿç”µåŠ›",
    "601166": "å…´ä¸šé“¶è¡Œ",
    "600028": "ä¸­å›½çŸ³åŒ–",
    "600674": "å·æŠ•èƒ½æº",
    "000919": "é‡‘é™µè¯ä¸š",
    "001206": "ä¾ä¾è‚¡ä»½",
    "002223": "é±¼è·ƒåŒ»ç–—",
}

logger = logging.getLogger(__name__)


class RateLimiter:
    """
    çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å™¨ï¼ˆä»¤ç‰Œæ¡¶ç®—æ³•ï¼‰

    ç”¨äºæ§åˆ¶ API è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è§¦å‘é™æµé”™è¯¯ï¼ˆå¦‚ 429ï¼‰ã€‚

    ç‰¹æ€§ï¼š
    - çº¿ç¨‹å®‰å…¨ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘è°ƒç”¨
    - æ»‘åŠ¨çª—å£ï¼šåŸºäºæ—¶é—´çª—å£çš„è¯·æ±‚è®¡æ•°
    - è‡ªåŠ¨ç­‰å¾…ï¼šå½“è¾¾åˆ°é™åˆ¶æ—¶è‡ªåŠ¨ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚
    - å¯é…ç½®ï¼šæ”¯æŒè‡ªå®šä¹‰æ¯åˆ†é’Ÿè¯·æ±‚æ•°å’Œæœ€å°é—´éš”

    ä½¿ç”¨ç¤ºä¾‹ï¼š
        limiter = RateLimiter(requests_per_minute=6, min_interval=10.0)
        limiter.wait_if_needed()  # è¯·æ±‚å‰è°ƒç”¨
        # ... æ‰§è¡Œ API è¯·æ±‚ ...
        limiter.record_request()  # è¯·æ±‚åè°ƒç”¨
    """

    def __init__(self, requests_per_minute: int = 6, min_interval: float = 10.0, enabled: bool = True):
        """
        åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨

        Args:
            requests_per_minute: æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ 6ï¼‰
            min_interval: è¯·æ±‚ä¹‹é—´çš„æœ€å°é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤ 10 ç§’ï¼‰
            enabled: æ˜¯å¦å¯ç”¨é™æµï¼ˆé»˜è®¤ Trueï¼Œè®¾ä¸º False åˆ™è·³è¿‡æ‰€æœ‰é™æµé€»è¾‘ï¼‰
        """
        self.requests_per_minute = requests_per_minute
        self.min_interval = min_interval
        self.enabled = enabled

        # ä½¿ç”¨ deque ç»´æŠ¤æœ€è¿‘è¯·æ±‚çš„æ—¶é—´æˆ³ï¼ˆçº¿ç¨‹å®‰å…¨éœ€è¦é…åˆé”ä½¿ç”¨ï¼‰
        self._request_timestamps: deque = deque(maxlen=requests_per_minute)
        self._lock = threading.Lock()  # ä¿æŠ¤å…±äº«çŠ¶æ€çš„é”
        self._last_request_time: float = 0.0  # æœ€åä¸€æ¬¡è¯·æ±‚çš„æ—¶é—´

    def wait_if_needed(self) -> None:
        """
        å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚

        æ£€æŸ¥é€»è¾‘ï¼š
        1. å¦‚æœæœªå¯ç”¨ï¼Œç›´æ¥è¿”å›
        2. æ£€æŸ¥è·ç¦»ä¸Šæ¬¡è¯·æ±‚æ˜¯å¦æ»¡è¶³æœ€å°é—´éš”
        3. æ£€æŸ¥æœ€è¿‘ 1 åˆ†é’Ÿå†…æ˜¯å¦å·²è¾¾åˆ°è¯·æ±‚ä¸Šé™
        4. å¦‚æœéœ€è¦ç­‰å¾…ï¼Œè®¡ç®—ç­‰å¾…æ—¶é—´å¹¶ sleep
        """
        if not self.enabled:
            return

        with self._lock:
            current_time = time.time()

            # æ¸…ç†è¶…è¿‡ 1 åˆ†é’Ÿçš„æ—¶é—´æˆ³
            one_minute_ago = current_time - 60.0
            while self._request_timestamps and self._request_timestamps[0] < one_minute_ago:
                self._request_timestamps.popleft()

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ¯åˆ†é’Ÿè¯·æ±‚ä¸Šé™
            if len(self._request_timestamps) >= self.requests_per_minute:
                # éœ€è¦ç­‰å¾…åˆ°æœ€æ—©è¯·æ±‚è¶…è¿‡ 1 åˆ†é’Ÿ
                oldest_request_time = self._request_timestamps[0]
                wait_time = 60.0 - (current_time - oldest_request_time) + 0.5  # é¢å¤– 0.5 ç§’ç¼“å†²
                if wait_time > 0:
                    logger.info(
                        f"[RateLimiter] è¾¾åˆ°æ¯åˆ†é’Ÿ {self.requests_per_minute} æ¬¡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’..."
                    )
                    time.sleep(wait_time)
                    current_time = time.time()  # æ›´æ–°å½“å‰æ—¶é—´

            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€å°é—´éš”
            if self._last_request_time > 0:
                time_since_last = current_time - self._last_request_time
                if time_since_last < self.min_interval:
                    wait_time = self.min_interval - time_since_last
                    logger.debug(
                        f"[RateLimiter] è·ç¦»ä¸Šæ¬¡è¯·æ±‚ {time_since_last:.1f} ç§’ï¼Œç­‰å¾… {wait_time:.1f} ç§’ä»¥æ»¡è¶³æœ€å°é—´éš”..."
                    )
                    time.sleep(wait_time)
                    current_time = time.time()  # æ›´æ–°å½“å‰æ—¶é—´

    def record_request(self) -> None:
        """
        è®°å½•ä¸€æ¬¡è¯·æ±‚ï¼ˆåœ¨è¯·æ±‚æˆåŠŸåè°ƒç”¨ï¼‰

        å°†å½“å‰æ—¶é—´æˆ³æ·»åŠ åˆ°è¯·æ±‚å†å²ä¸­ï¼Œç”¨äºåç»­çš„é€Ÿç‡é™åˆ¶è®¡ç®—ã€‚
        """
        if not self.enabled:
            return

        with self._lock:
            current_time = time.time()
            self._request_timestamps.append(current_time)
            self._last_request_time = current_time

    def reset(self) -> None:
        """é‡ç½®é™æµå™¨çŠ¶æ€ï¼ˆæ¸…ç©ºè¯·æ±‚å†å²ï¼‰"""
        with self._lock:
            self._request_timestamps.clear()
            self._last_request_time = 0.0

    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰é™æµå™¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        with self._lock:
            current_time = time.time()
            one_minute_ago = current_time - 60.0

            # æ¸…ç†è¿‡æœŸæ—¶é—´æˆ³
            valid_timestamps = [ts for ts in self._request_timestamps if ts >= one_minute_ago]

            return {
                "enabled": self.enabled,
                "requests_per_minute": self.requests_per_minute,
                "min_interval": self.min_interval,
                "requests_in_last_minute": len(valid_timestamps),
                "last_request_time": self._last_request_time,
                "time_since_last_request": (
                    current_time - self._last_request_time if self._last_request_time > 0 else None
                ),
            }


class GeminiAnalyzer:
    """
    Gemini AI åˆ†æå™¨

    èŒè´£ï¼š
    1. è°ƒç”¨ Google Gemini API è¿›è¡Œè‚¡ç¥¨åˆ†æ
    2. ç»“åˆé¢„å…ˆæœç´¢çš„æ–°é—»å’ŒæŠ€æœ¯é¢æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Š
    3. è§£æ AI è¿”å›çš„ JSON æ ¼å¼ç»“æœ

    ä½¿ç”¨æ–¹å¼ï¼š
        analyzer = GeminiAnalyzer()
        result = analyzer.analyze(context, news_context)
    """

    # ä½¿ç”¨ä»promptsæ¨¡å—å¯¼å…¥çš„SYSTEM_PROMPT
    SYSTEM_PROMPT = SYSTEM_PROMPT

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– AI åˆ†æå™¨

        ä¼˜å…ˆçº§ï¼šGemini > OpenAI å…¼å®¹ API

        Args:
            api_key: Gemini API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        config = get_config()
        self._api_key = api_key or config.gemini_api_key
        self._model = None
        self._current_model_name = None  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°
        self._using_fallback = False  # æ˜¯å¦æ­£åœ¨ä½¿ç”¨å¤‡é€‰æ¨¡å‹
        self._use_openai = False  # æ˜¯å¦ä½¿ç”¨ OpenAI å…¼å®¹ API
        self._openai_client = None  # OpenAI å®¢æˆ·ç«¯

        # åˆå§‹åŒ–è§£æå™¨
        self._parser = DashboardParser()

        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ï¼ˆå¯é€‰ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ï¼‰
        self._rate_limiter: Optional[RateLimiter] = None
        if config.gemini_rate_limit_enabled:
            self._rate_limiter = RateLimiter(
                requests_per_minute=config.gemini_rate_limit_per_minute,
                min_interval=config.gemini_rate_limit_min_interval,
                enabled=True,
            )
            logger.info(
                f"[RateLimiter] å·²å¯ç”¨é€Ÿç‡é™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š {config.gemini_rate_limit_per_minute} æ¬¡è¯·æ±‚ï¼Œ"
                f"æœ€å°é—´éš” {config.gemini_rate_limit_min_interval} ç§’"
            )
        else:
            logger.debug("[RateLimiter] é€Ÿç‡é™åˆ¶å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰çš„è¯·æ±‚å»¶è¿Ÿæœºåˆ¶")

        # æ£€æŸ¥ Gemini API Key æ˜¯å¦æœ‰æ•ˆï¼ˆè¿‡æ»¤å ä½ç¬¦ï¼‰
        gemini_key_valid = self._api_key and not self._api_key.startswith("your_") and len(self._api_key) > 10

        # ä¼˜å…ˆå°è¯•åˆå§‹åŒ– Gemini
        if gemini_key_valid:
            try:
                self._init_model()
            except Exception as e:
                logger.warning(f"Gemini åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯• OpenAI å…¼å®¹ API")
                self._init_openai_fallback()
        else:
            # Gemini Key æœªé…ç½®ï¼Œå°è¯• OpenAI
            logger.info("Gemini API Key æœªé…ç½®ï¼Œå°è¯•ä½¿ç”¨ OpenAI å…¼å®¹ API")
            self._init_openai_fallback()

        # ä¸¤è€…éƒ½æœªé…ç½®
        if not self._model and not self._openai_client:
            logger.warning("æœªé…ç½®ä»»ä½• AI API Keyï¼ŒAI åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

    def _init_openai_fallback(self) -> None:
        """
        åˆå§‹åŒ– OpenAI å…¼å®¹ API ä½œä¸ºå¤‡é€‰

        æ”¯æŒæ‰€æœ‰ OpenAI æ ¼å¼çš„ APIï¼ŒåŒ…æ‹¬ï¼š
        - OpenAI å®˜æ–¹
        - DeepSeek
        - é€šä¹‰åƒé—®
        - Moonshot ç­‰
        """
        config = get_config()

        # æ£€æŸ¥ OpenAI API Key æ˜¯å¦æœ‰æ•ˆï¼ˆè¿‡æ»¤å ä½ç¬¦ï¼‰
        openai_key_valid = (
            config.openai_api_key and not config.openai_api_key.startswith("your_") and len(config.openai_api_key) > 10
        )

        if not openai_key_valid:
            logger.debug("OpenAI å…¼å®¹ API æœªé…ç½®æˆ–é…ç½®æ— æ•ˆ")
            return

        # åˆ†ç¦» import å’Œå®¢æˆ·ç«¯åˆ›å»ºï¼Œä»¥ä¾¿æä¾›æ›´å‡†ç¡®çš„é”™è¯¯ä¿¡æ¯
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("æœªå®‰è£… openai åº“ï¼Œè¯·è¿è¡Œ: pip install openai")
            return

        try:
            # base_url å¯é€‰ï¼Œä¸å¡«åˆ™ä½¿ç”¨ OpenAI å®˜æ–¹é»˜è®¤åœ°å€
            client_kwargs = {"api_key": config.openai_api_key}
            if config.openai_base_url and config.openai_base_url.startswith("http"):
                client_kwargs["base_url"] = config.openai_base_url

            self._openai_client = OpenAI(**client_kwargs)
            self._current_model_name = config.openai_model
            self._use_openai = True
            logger.info(
                f"OpenAI å…¼å®¹ API åˆå§‹åŒ–æˆåŠŸ (base_url: {config.openai_base_url}, model: {config.openai_model})"
            )
        except ImportError as e:
            # ä¾èµ–ç¼ºå¤±ï¼ˆå¦‚ socksioï¼‰
            if "socksio" in str(e).lower() or "socks" in str(e).lower():
                logger.error(
                    f"OpenAI å®¢æˆ·ç«¯éœ€è¦ SOCKS ä»£ç†æ”¯æŒï¼Œè¯·è¿è¡Œ: pip install httpx[socks] æˆ– pip install socksio"
                )
            else:
                logger.error(f"OpenAI ä¾èµ–ç¼ºå¤±: {e}")
        except Exception as e:
            error_msg = str(e).lower()
            if "socks" in error_msg or "socksio" in error_msg or "proxy" in error_msg:
                logger.error(f"OpenAI ä»£ç†é…ç½®é”™è¯¯: {e}ï¼Œå¦‚ä½¿ç”¨ SOCKS ä»£ç†è¯·è¿è¡Œ: pip install httpx[socks]")
            else:
                logger.error(f"OpenAI å…¼å®¹ API åˆå§‹åŒ–å¤±è´¥: {e}")

    def _init_model(self) -> None:
        """
        åˆå§‹åŒ– Gemini æ¨¡å‹

        é…ç½®ï¼š
        - ä½¿ç”¨ gemini-3-flash-preview æˆ– gemini-2.5-flash æ¨¡å‹
        - ä¸å¯ç”¨ Google Searchï¼ˆä½¿ç”¨å¤–éƒ¨ Tavily/SerpAPI æœç´¢ï¼‰
        """
        try:
            import google.generativeai as genai

            # é…ç½® API Key
            genai.configure(api_key=self._api_key)

            # ä»é…ç½®è·å–æ¨¡å‹åç§°
            config = get_config()
            model_name = config.gemini_model
            fallback_model = config.gemini_model_fallback

            # ä¸å†ä½¿ç”¨ Google Search Groundingï¼ˆå·²çŸ¥æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
            # æ”¹ä¸ºä½¿ç”¨å¤–éƒ¨æœç´¢æœåŠ¡ï¼ˆTavily/SerpAPIï¼‰é¢„å…ˆè·å–æ–°é—»

            # å°è¯•åˆå§‹åŒ–ä¸»æ¨¡å‹
            try:
                self._model = genai.GenerativeModel(
                    model_name=model_name,
                    system_instruction=self.SYSTEM_PROMPT,
                )
                self._current_model_name = model_name
                self._using_fallback = False
                logger.info(f"Gemini æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {model_name})")
            except Exception as model_error:
                # å°è¯•å¤‡é€‰æ¨¡å‹
                logger.warning(f"ä¸»æ¨¡å‹ {model_name} åˆå§‹åŒ–å¤±è´¥: {model_error}ï¼Œå°è¯•å¤‡é€‰æ¨¡å‹ {fallback_model}")
                self._model = genai.GenerativeModel(
                    model_name=fallback_model,
                    system_instruction=self.SYSTEM_PROMPT,
                )
                self._current_model_name = fallback_model
                self._using_fallback = True
                logger.info(f"Gemini å¤‡é€‰æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {fallback_model})")

        except Exception as e:
            logger.error(f"Gemini æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self._model = None

    def _switch_to_fallback_model(self) -> bool:
        """
        åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹

        Returns:
            æ˜¯å¦æˆåŠŸåˆ‡æ¢
        """
        try:
            import google.generativeai as genai

            config = get_config()
            fallback_model = config.gemini_model_fallback

            logger.warning(f"[LLM] åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹: {fallback_model}")
            self._model = genai.GenerativeModel(
                model_name=fallback_model,
                system_instruction=self.SYSTEM_PROMPT,
            )
            self._current_model_name = fallback_model
            self._using_fallback = True
            logger.info(f"[LLM] å¤‡é€‰æ¨¡å‹ {fallback_model} åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"[LLM] åˆ‡æ¢å¤‡é€‰æ¨¡å‹å¤±è´¥: {e}")
            return False

    def is_available(self) -> bool:
        """æ£€æŸ¥åˆ†æå™¨æ˜¯å¦å¯ç”¨"""
        return self._model is not None or self._openai_client is not None

    def _call_openai_api(self, prompt: str, generation_config: dict) -> str:
        """
        è°ƒç”¨ OpenAI å…¼å®¹ API

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            å“åº”æ–‡æœ¬
        """
        config = get_config()
        max_retries = config.gemini_max_retries
        base_delay = config.gemini_retry_delay

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    delay = min(delay, 60)
                    logger.info(f"[OpenAI] ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)

                # ä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼ŒOpenAI API ä¹Ÿå—é™åˆ¶ï¼‰
                if self._rate_limiter:
                    self._rate_limiter.wait_if_needed()

                response = self._openai_client.chat.completions.create(
                    model=self._current_model_name,
                    messages=[{"role": "system", "content": self.SYSTEM_PROMPT}, {"role": "user", "content": prompt}],
                    temperature=generation_config.get("temperature", 0.7),
                    max_tokens=generation_config.get("max_output_tokens", 8192),
                )

                if response and response.choices and response.choices[0].message.content:
                    # è¯·æ±‚æˆåŠŸåè®°å½•ï¼ˆç”¨äºé€Ÿç‡é™åˆ¶ï¼‰
                    if self._rate_limiter:
                        self._rate_limiter.record_request()
                    return response.choices[0].message.content
                else:
                    raise ValueError("OpenAI API è¿”å›ç©ºå“åº”")

            except Exception as e:
                error_str = str(e)
                is_rate_limit = "429" in error_str or "rate" in error_str.lower() or "quota" in error_str.lower()

                if is_rate_limit:
                    logger.warning(f"[OpenAI] API é™æµï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")
                else:
                    logger.warning(f"[OpenAI] API è°ƒç”¨å¤±è´¥ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

                if attempt == max_retries - 1:
                    raise

        raise Exception("OpenAI API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")

    def _call_api_with_retry(self, prompt: str, generation_config: dict) -> str:
        """
        è°ƒç”¨ AI APIï¼Œå¸¦æœ‰é‡è¯•å’Œæ¨¡å‹åˆ‡æ¢æœºåˆ¶

        ä¼˜å…ˆçº§ï¼šGemini > Gemini å¤‡é€‰æ¨¡å‹ > OpenAI å…¼å®¹ API

        å¤„ç† 429 é™æµé”™è¯¯ï¼š
        1. å…ˆæŒ‡æ•°é€€é¿é‡è¯•
        2. å¤šæ¬¡å¤±è´¥ååˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
        3. Gemini å®Œå…¨å¤±è´¥åå°è¯• OpenAI

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            å“åº”æ–‡æœ¬
        """
        # å¦‚æœå·²ç»åœ¨ä½¿ç”¨ OpenAI æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨ OpenAI
        if self._use_openai:
            return self._call_openai_api(prompt, generation_config)

        config = get_config()
        max_retries = config.gemini_max_retries
        base_delay = config.gemini_retry_delay

        last_error = None
        tried_fallback = getattr(self, "_using_fallback", False)

        for attempt in range(max_retries):
            try:
                # è¯·æ±‚å‰å¢åŠ å»¶æ—¶ï¼ˆé˜²æ­¢è¯·æ±‚è¿‡å¿«è§¦å‘é™æµï¼‰
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿: 5, 10, 20, 40...
                    delay = min(delay, 60)  # æœ€å¤§60ç§’
                    logger.info(f"[Gemini] ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)

                # ä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self._rate_limiter:
                    self._rate_limiter.wait_if_needed()

                response = self._model.generate_content(
                    prompt, generation_config=generation_config, request_options={"timeout": 120}
                )

                if response and response.text:
                    # è¯·æ±‚æˆåŠŸåè®°å½•ï¼ˆç”¨äºé€Ÿç‡é™åˆ¶ï¼‰
                    if self._rate_limiter:
                        self._rate_limiter.record_request()
                    return response.text
                else:
                    raise ValueError("Gemini è¿”å›ç©ºå“åº”")

            except Exception as e:
                last_error = e
                error_str = str(e)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ 429 é™æµé”™è¯¯
                is_rate_limit = "429" in error_str or "quota" in error_str.lower() or "rate" in error_str.lower()

                if is_rate_limit:
                    logger.warning(f"[Gemini] API é™æµ (429)ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

                    # å¦‚æœå·²ç»é‡è¯•äº†ä¸€åŠæ¬¡æ•°ä¸”è¿˜æ²¡åˆ‡æ¢è¿‡å¤‡é€‰æ¨¡å‹ï¼Œå°è¯•åˆ‡æ¢
                    if attempt >= max_retries // 2 and not tried_fallback:
                        if self._switch_to_fallback_model():
                            tried_fallback = True
                            logger.info("[Gemini] å·²åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ï¼Œç»§ç»­é‡è¯•")
                        else:
                            logger.warning("[Gemini] åˆ‡æ¢å¤‡é€‰æ¨¡å‹å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹é‡è¯•")
                else:
                    # éé™æµé”™è¯¯ï¼Œè®°å½•å¹¶ç»§ç»­é‡è¯•
                    logger.warning(f"[Gemini] API è°ƒç”¨å¤±è´¥ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

        # Gemini æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯• OpenAI å…¼å®¹ API
        if self._openai_client:
            logger.warning("[Gemini] æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œåˆ‡æ¢åˆ° OpenAI å…¼å®¹ API")
            try:
                return self._call_openai_api(prompt, generation_config)
            except Exception as openai_error:
                logger.error(f"[OpenAI] å¤‡é€‰ API ä¹Ÿå¤±è´¥: {openai_error}")
                raise last_error or openai_error
        elif config.openai_api_key and config.openai_base_url:
            # å°è¯•æ‡’åŠ è½½åˆå§‹åŒ– OpenAI
            logger.warning("[Gemini] æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå°è¯•åˆå§‹åŒ– OpenAI å…¼å®¹ API")
            self._init_openai_fallback()
            if self._openai_client:
                try:
                    return self._call_openai_api(prompt, generation_config)
                except Exception as openai_error:
                    logger.error(f"[OpenAI] å¤‡é€‰ API ä¹Ÿå¤±è´¥: {openai_error}")
                    raise last_error or openai_error

        # æ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
        raise last_error or Exception("æ‰€æœ‰ AI API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")

    def analyze(self, context: Dict[str, Any], news_context: Optional[str] = None) -> AnalysisResult:
        """
        åˆ†æå•åªè‚¡ç¥¨

        æµç¨‹ï¼š
        1. æ ¼å¼åŒ–è¾“å…¥æ•°æ®ï¼ˆæŠ€æœ¯é¢ + æ–°é—»ï¼‰
        2. è°ƒç”¨ Gemini APIï¼ˆå¸¦é‡è¯•å’Œæ¨¡å‹åˆ‡æ¢ï¼‰
        3. è§£æ JSON å“åº”
        4. è¿”å›ç»“æ„åŒ–ç»“æœ

        Args:
            context: ä» storage.get_analysis_context() è·å–çš„ä¸Šä¸‹æ–‡æ•°æ®
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹ï¼ˆå¯é€‰ï¼‰

        Returns:
            AnalysisResult å¯¹è±¡
        """
        code = context.get("code", "Unknown")
        config = get_config()

        # è¯·æ±‚å‰å¢åŠ å»¶æ—¶ï¼ˆé˜²æ­¢è¿ç»­è¯·æ±‚è§¦å‘é™æµï¼‰
        request_delay = config.gemini_request_delay
        if request_delay > 0:
            logger.debug(f"[LLM] è¯·æ±‚å‰ç­‰å¾… {request_delay:.1f} ç§’...")
            time.sleep(request_delay)

        # ä¼˜å…ˆä»ä¸Šä¸‹æ–‡è·å–è‚¡ç¥¨åç§°ï¼ˆç”± main.py ä¼ å…¥ï¼‰
        name = context.get("stock_name")
        if not name or name.startswith("è‚¡ç¥¨"):
            # å¤‡é€‰ï¼šä» realtime ä¸­è·å–
            if "realtime" in context and context["realtime"].get("name"):
                name = context["realtime"]["name"]
            else:
                # æœ€åä»æ˜ å°„è¡¨è·å–
                name = STOCK_NAME_MAP.get(code, f"è‚¡ç¥¨{code}")

        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤ç»“æœ
        if not self.is_available():
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary="AI åˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰",
                risk_warning="è¯·é…ç½® Gemini API Key åé‡è¯•",
                success=False,
                error_message="Gemini API Key æœªé…ç½®",
            )

        try:
            # æ ¼å¼åŒ–è¾“å…¥ï¼ˆåŒ…å«æŠ€æœ¯é¢æ•°æ®å’Œæ–°é—»ï¼‰
            prompt = self._format_prompt(context, name, news_context)

            # è·å–æ¨¡å‹åç§°
            model_name = getattr(self, "_current_model_name", None)
            if not model_name:
                model_name = getattr(self._model, "_model_name", "unknown")
                if hasattr(self._model, "model_name"):
                    model_name = self._model.model_name

            logger.info(f"========== AI åˆ†æ {name}({code}) ==========")
            logger.info(f"[LLMé…ç½®] æ¨¡å‹: {model_name}")
            logger.info(f"[LLMé…ç½®] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"[LLMé…ç½®] æ˜¯å¦åŒ…å«æ–°é—»: {'æ˜¯' if news_context else 'å¦'}")

            # è®°å½•å®Œæ•´ prompt åˆ°æ—¥å¿—ï¼ˆINFOçº§åˆ«è®°å½•æ‘˜è¦ï¼ŒDEBUGè®°å½•å®Œæ•´ï¼‰
            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
            logger.info(f"[LLM Prompt é¢„è§ˆ]\n{prompt_preview}")
            logger.debug(f"=== å®Œæ•´ Prompt ({len(prompt)}å­—ç¬¦) ===\n{prompt}\n=== End Prompt ===")

            # è®¾ç½®ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": 0.7,
                "max_output_tokens": 8192,
            }

            logger.info(
                f"[LLMè°ƒç”¨] å¼€å§‹è°ƒç”¨ Gemini API (temperature={generation_config['temperature']}, max_tokens={generation_config['max_output_tokens']})..."
            )

            # ä½¿ç”¨å¸¦é‡è¯•çš„ API è°ƒç”¨
            start_time = time.time()
            response_text = self._call_api_with_retry(prompt, generation_config)
            elapsed = time.time() - start_time

            # è®°å½•å“åº”ä¿¡æ¯
            logger.info(f"[LLMè¿”å›] Gemini API å“åº”æˆåŠŸ, è€—æ—¶ {elapsed:.2f}s, å“åº”é•¿åº¦ {len(response_text)} å­—ç¬¦")

            # è®°å½•å“åº”é¢„è§ˆï¼ˆINFOçº§åˆ«ï¼‰å’Œå®Œæ•´å“åº”ï¼ˆDEBUGçº§åˆ«ï¼‰
            response_preview = response_text[:300] + "..." if len(response_text) > 300 else response_text
            logger.info(f"[LLMè¿”å› é¢„è§ˆ]\n{response_preview}")
            logger.debug(f"=== Gemini å®Œæ•´å“åº” ({len(response_text)}å­—ç¬¦) ===\n{response_text}\n=== End Response ===")

            # è§£æå“åº”ï¼ˆä½¿ç”¨è§£æå™¨ï¼‰
            result = self._parser.parse(response_text, code, name)
            result.raw_response = response_text
            result.search_performed = bool(news_context)

            logger.info(f"[LLMè§£æ] {name}({code}) åˆ†æå®Œæˆ: {result.trend_prediction}, è¯„åˆ† {result.sentiment_score}")

            return result

        except Exception as e:
            logger.error(f"AI åˆ†æ {name}({code}) å¤±è´¥: {e}")
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)[:100]}",
                risk_warning="åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨åˆ†æ",
                success=False,
                error_message=str(e),
            )

    def _format_prompt(self, context: Dict[str, Any], name: str, news_context: Optional[str] = None) -> str:
        """
        æ ¼å¼åŒ–åˆ†ææç¤ºè¯ï¼ˆå†³ç­–ä»ªè¡¨ç›˜ v2.0ï¼‰

        åŒ…å«ï¼šæŠ€æœ¯æŒ‡æ ‡ã€å®æ—¶è¡Œæƒ…ï¼ˆé‡æ¯”/æ¢æ‰‹ç‡ï¼‰ã€ç­¹ç åˆ†å¸ƒã€è¶‹åŠ¿åˆ†æã€æ–°é—»

        Args:
            context: æŠ€æœ¯é¢æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å¢å¼ºæ•°æ®ï¼‰
            name: è‚¡ç¥¨åç§°ï¼ˆé»˜è®¤å€¼ï¼Œå¯èƒ½è¢«ä¸Šä¸‹æ–‡è¦†ç›–ï¼‰
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹
        """
        code = context.get("code", "Unknown")

        # ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„è‚¡ç¥¨åç§°ï¼ˆä» realtime_quote è·å–ï¼‰
        stock_name = context.get("stock_name", name)
        if not stock_name or stock_name == f"è‚¡ç¥¨{code}":
            stock_name = STOCK_NAME_MAP.get(code, f"è‚¡ç¥¨{code}")

        today = context.get("today", {})

        # ========== æ„å»ºå†³ç­–ä»ªè¡¨ç›˜æ ¼å¼çš„è¾“å…¥ ==========
        prompt = f"""# å†³ç­–ä»ªè¡¨ç›˜åˆ†æè¯·æ±‚

## ğŸ“Š è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
| é¡¹ç›® | æ•°æ® |
|------|------|
| è‚¡ç¥¨ä»£ç  | **{code}** |
| è‚¡ç¥¨åç§° | **{stock_name}** |
| åˆ†ææ—¥æœŸ | {context.get('date', 'æœªçŸ¥')} |

---

## ğŸ“ˆ æŠ€æœ¯é¢æ•°æ®

### ä»Šæ—¥è¡Œæƒ…
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ”¶ç›˜ä»· | {today.get('close', 'N/A')} å…ƒ |
| å¼€ç›˜ä»· | {today.get('open', 'N/A')} å…ƒ |
| æœ€é«˜ä»· | {today.get('high', 'N/A')} å…ƒ |
| æœ€ä½ä»· | {today.get('low', 'N/A')} å…ƒ |
| æ¶¨è·Œå¹… | {today.get('pct_chg', 'N/A')}% |
| æˆäº¤é‡ | {self._format_volume(today.get('volume'))} |
| æˆäº¤é¢ | {self._format_amount(today.get('amount'))} |

### å‡çº¿ç³»ç»Ÿï¼ˆå…³é”®åˆ¤æ–­æŒ‡æ ‡ï¼‰
| å‡çº¿ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| MA5 | {today.get('ma5', 'N/A')} | çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA10 | {today.get('ma10', 'N/A')} | ä¸­çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA20 | {today.get('ma20', 'N/A')} | ä¸­æœŸè¶‹åŠ¿çº¿ |
| å‡çº¿å½¢æ€ | {context.get('ma_status', 'æœªçŸ¥')} | å¤šå¤´/ç©ºå¤´/ç¼ ç»• |
"""

        # æ·»åŠ å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆé‡æ¯”ã€æ¢æ‰‹ç‡ç­‰ï¼‰
        if "realtime" in context:
            rt = context["realtime"]
            prompt += f"""
### å®æ—¶è¡Œæƒ…å¢å¼ºæ•°æ®
| æŒ‡æ ‡ | æ•°å€¼ | è§£è¯» |
|------|------|------|
| å½“å‰ä»·æ ¼ | {rt.get('price', 'N/A')} å…ƒ | |
| **é‡æ¯”** | **{rt.get('volume_ratio', 'N/A')}** | {rt.get('volume_ratio_desc', '')} |
| **æ¢æ‰‹ç‡** | **{rt.get('turnover_rate', 'N/A')}%** | |
| å¸‚ç›ˆç‡(åŠ¨æ€) | {rt.get('pe_ratio', 'N/A')} | |
| å¸‚å‡€ç‡ | {rt.get('pb_ratio', 'N/A')} | |
| æ€»å¸‚å€¼ | {self._format_amount(rt.get('total_mv'))} | |
| æµé€šå¸‚å€¼ | {self._format_amount(rt.get('circ_mv'))} | |
| 60æ—¥æ¶¨è·Œå¹… | {rt.get('change_60d', 'N/A')}% | ä¸­æœŸè¡¨ç° |
"""

        # æ·»åŠ ç­¹ç åˆ†å¸ƒæ•°æ®
        if "chip" in context:
            chip = context["chip"]
            profit_ratio = chip.get("profit_ratio", 0)
            prompt += f"""
### ç­¹ç åˆ†å¸ƒæ•°æ®ï¼ˆæ•ˆç‡æŒ‡æ ‡ï¼‰
| æŒ‡æ ‡ | æ•°å€¼ | å¥åº·æ ‡å‡† |
|------|------|----------|
| **è·åˆ©æ¯”ä¾‹** | **{profit_ratio:.1%}** | 70-90%æ—¶è­¦æƒ• |
| å¹³å‡æˆæœ¬ | {chip.get('avg_cost', 'N/A')} å…ƒ | ç°ä»·åº”é«˜äº5-15% |
| 90%ç­¹ç é›†ä¸­åº¦ | {chip.get('concentration_90', 0):.2%} | <15%ä¸ºé›†ä¸­ |
| 70%ç­¹ç é›†ä¸­åº¦ | {chip.get('concentration_70', 0):.2%} | |
| ç­¹ç çŠ¶æ€ | {chip.get('chip_status', 'æœªçŸ¥')} | |
"""

        # æ·»åŠ è¶‹åŠ¿åˆ†æç»“æœï¼ˆåŸºäºäº¤æ˜“ç†å¿µçš„é¢„åˆ¤ï¼‰
        if "trend_analysis" in context:
            trend = context["trend_analysis"]
            bias_warning = "ğŸš¨ è¶…è¿‡5%ï¼Œä¸¥ç¦è¿½é«˜ï¼" if trend.get("bias_ma5", 0) > 5 else "âœ… å®‰å…¨èŒƒå›´"
            prompt += f"""
### è¶‹åŠ¿åˆ†æé¢„åˆ¤ï¼ˆåŸºäºäº¤æ˜“ç†å¿µï¼‰
| æŒ‡æ ‡ | æ•°å€¼ | åˆ¤å®š |
|------|------|------|
| è¶‹åŠ¿çŠ¶æ€ | {trend.get('trend_status', 'æœªçŸ¥')} | |
| å‡çº¿æ’åˆ— | {trend.get('ma_alignment', 'æœªçŸ¥')} | MA5>MA10>MA20ä¸ºå¤šå¤´ |
| è¶‹åŠ¿å¼ºåº¦ | {trend.get('trend_strength', 0)}/100 | |
| **ä¹–ç¦»ç‡(MA5)** | **{trend.get('bias_ma5', 0):+.2f}%** | {bias_warning} |
| ä¹–ç¦»ç‡(MA10) | {trend.get('bias_ma10', 0):+.2f}% | |
| é‡èƒ½çŠ¶æ€ | {trend.get('volume_status', 'æœªçŸ¥')} | {trend.get('volume_trend', '')} |
| ç³»ç»Ÿä¿¡å· | {trend.get('buy_signal', 'æœªçŸ¥')} | |
| ç³»ç»Ÿè¯„åˆ† | {trend.get('signal_score', 0)}/100 | |

#### ç³»ç»Ÿåˆ†æç†ç”±
**ä¹°å…¥ç†ç”±**ï¼š
{chr(10).join('- ' + r for r in trend.get('signal_reasons', ['æ— '])) if trend.get('signal_reasons') else '- æ— '}

**é£é™©å› ç´ **ï¼š
{chr(10).join('- ' + r for r in trend.get('risk_factors', ['æ— '])) if trend.get('risk_factors') else '- æ— '}
"""

        # æ·»åŠ æ˜¨æ—¥å¯¹æ¯”æ•°æ®
        if "yesterday" in context:
            volume_change = context.get("volume_change_ratio", "N/A")
            prompt += f"""
### é‡ä»·å˜åŒ–
- æˆäº¤é‡è¾ƒæ˜¨æ—¥å˜åŒ–ï¼š{volume_change}å€
- ä»·æ ¼è¾ƒæ˜¨æ—¥å˜åŒ–ï¼š{context.get('price_change_ratio', 'N/A')}%
"""

        # æ·»åŠ æ–°é—»æœç´¢ç»“æœï¼ˆé‡ç‚¹åŒºåŸŸï¼‰
        prompt += """
---

## ğŸ“° èˆ†æƒ…æƒ…æŠ¥
"""
        if news_context:
            prompt += f"""
ä»¥ä¸‹æ˜¯ **{stock_name}({code})** è¿‘7æ—¥çš„æ–°é—»æœç´¢ç»“æœï¼Œè¯·é‡ç‚¹æå–ï¼š
1. ğŸš¨ **é£é™©è­¦æŠ¥**ï¼šå‡æŒã€å¤„ç½šã€åˆ©ç©º
2. ğŸ¯ **åˆ©å¥½å‚¬åŒ–**ï¼šä¸šç»©ã€åˆåŒã€æ”¿ç­–
3. ğŸ“Š **ä¸šç»©é¢„æœŸ**ï¼šå¹´æŠ¥é¢„å‘Šã€ä¸šç»©å¿«æŠ¥

```
{news_context}
```
"""
        else:
            prompt += """
æœªæœç´¢åˆ°è¯¥è‚¡ç¥¨è¿‘æœŸçš„ç›¸å…³æ–°é—»ã€‚è¯·ä¸»è¦ä¾æ®æŠ€æœ¯é¢æ•°æ®è¿›è¡Œåˆ†æã€‚
"""

        # æ˜ç¡®çš„è¾“å‡ºè¦æ±‚
        prompt += f"""
---

## âœ… åˆ†æä»»åŠ¡

è¯·ä¸º **{stock_name}({code})** ç”Ÿæˆã€å†³ç­–ä»ªè¡¨ç›˜ã€‘ï¼Œä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºã€‚

### é‡ç‚¹å…³æ³¨ï¼ˆå¿…é¡»æ˜ç¡®å›ç­”ï¼‰ï¼š
1. â“ æ˜¯å¦æ»¡è¶³ MA5>MA10>MA20 å¤šå¤´æ’åˆ—ï¼Ÿ
2. â“ å½“å‰ä¹–ç¦»ç‡æ˜¯å¦åœ¨å®‰å…¨èŒƒå›´å†…ï¼ˆ<5%ï¼‰ï¼Ÿâ€”â€” è¶…è¿‡5%å¿…é¡»æ ‡æ³¨"ä¸¥ç¦è¿½é«˜"
3. â“ é‡èƒ½æ˜¯å¦é…åˆï¼ˆç¼©é‡å›è°ƒ/æ”¾é‡çªç ´ï¼‰ï¼Ÿ
4. â“ ç­¹ç ç»“æ„æ˜¯å¦å¥åº·ï¼Ÿ
5. â“ æ¶ˆæ¯é¢æœ‰æ— é‡å¤§åˆ©ç©ºï¼Ÿï¼ˆå‡æŒã€å¤„ç½šã€ä¸šç»©å˜è„¸ç­‰ï¼‰

### å†³ç­–ä»ªè¡¨ç›˜è¦æ±‚ï¼š
- **æ ¸å¿ƒç»“è®º**ï¼šä¸€å¥è¯è¯´æ¸…è¯¥ä¹°/è¯¥å–/è¯¥ç­‰
- **æŒä»“åˆ†ç±»å»ºè®®**ï¼šç©ºä»“è€…æ€ä¹ˆåš vs æŒä»“è€…æ€ä¹ˆåš
- **å…·ä½“ç‹™å‡»ç‚¹ä½**ï¼šä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»·ï¼ˆç²¾ç¡®åˆ°åˆ†ï¼‰
- **æ£€æŸ¥æ¸…å•**ï¼šæ¯é¡¹ç”¨ âœ…/âš ï¸/âŒ æ ‡è®°

è¯·è¾“å‡ºå®Œæ•´çš„ JSON æ ¼å¼å†³ç­–ä»ªè¡¨ç›˜ã€‚"""

        return prompt

    def _format_volume(self, volume: Optional[float]) -> str:
        """æ ¼å¼åŒ–æˆäº¤é‡æ˜¾ç¤º"""
        if volume is None:
            return "N/A"
        if volume >= 1e8:
            return f"{volume / 1e8:.2f} äº¿è‚¡"
        elif volume >= 1e4:
            return f"{volume / 1e4:.2f} ä¸‡è‚¡"
        else:
            return f"{volume:.0f} è‚¡"

    def _format_amount(self, amount: Optional[float]) -> str:
        """æ ¼å¼åŒ–æˆäº¤é¢æ˜¾ç¤º"""
        if amount is None:
            return "N/A"
        if amount >= 1e8:
            return f"{amount / 1e8:.2f} äº¿å…ƒ"
        elif amount >= 1e4:
            return f"{amount / 1e4:.2f} ä¸‡å…ƒ"
        else:
            return f"{amount:.0f} å…ƒ"

    def analyze_gold(self, context: Dict[str, Any], news_context: Optional[str] = None) -> AnalysisResult:
        """
        åˆ†æé»„é‡‘

        ä½¿ç”¨ä¸“é—¨çš„é»„é‡‘åˆ†æ Prompt å’Œæ ¼å¼åŒ–æ–¹æ³•

        Args:
            context: é»„é‡‘æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆä»·æ ¼ã€æŠ€æœ¯æŒ‡æ ‡ç­‰ï¼‰
            news_context: æ–°é—»/èµ„è®¯ä¸Šä¸‹æ–‡ï¼ˆç¾è”å‚¨æ”¿ç­–ã€é€šèƒ€æ•°æ®ã€åœ°ç¼˜æ”¿æ²»ç­‰ï¼‰

        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        code = context.get("code", "AU")
        gold_name = context.get("gold_name", "é»„é‡‘")

        config = get_config()

        # è¯·æ±‚å‰å¢åŠ å»¶æ—¶ï¼ˆé˜²æ­¢è¿ç»­è¯·æ±‚è§¦å‘é™æµï¼‰
        request_delay = config.gemini_request_delay
        if request_delay > 0:
            logger.debug(f"[LLM] è¯·æ±‚å‰ç­‰å¾… {request_delay:.1f} ç§’...")
            time.sleep(request_delay)

        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤ç»“æœ
        if not self.is_available():
            return AnalysisResult(
                code=code,
                name=gold_name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary="AI åˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰",
                risk_warning="è¯·é…ç½® Gemini API Key åé‡è¯•",
                success=False,
                error_message="Gemini API Key æœªé…ç½®",
            )

        try:
            # æ ¼å¼åŒ–é»„é‡‘åˆ†ææç¤ºè¯
            prompt = self._format_gold_prompt(context, gold_name, news_context)

            # è·å–æ¨¡å‹åç§°
            model_name = getattr(self, "_current_model_name", None)
            if not model_name:
                model_name = getattr(self._model, "_model_name", "unknown")
                if hasattr(self._model, "model_name"):
                    model_name = self._model.model_name

            logger.info(f"========== AI åˆ†æé»„é‡‘ {gold_name}({code}) ==========")
            logger.info(f"[LLMé…ç½®] æ¨¡å‹: {model_name}")
            logger.info(f"[LLMé…ç½®] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"[LLMé…ç½®] æ˜¯å¦åŒ…å«æ–°é—»: {'æ˜¯' if news_context else 'å¦'}")

            # è®°å½•å®Œæ•´ prompt åˆ°æ—¥å¿—
            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
            logger.info(f"[LLM Prompt é¢„è§ˆ]\n{prompt_preview}")
            logger.debug(f"=== å®Œæ•´ Prompt ({len(prompt)}å­—ç¬¦) ===\n{prompt}\n=== End Prompt ===")

            # è®¾ç½®ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": 0.7,
                "max_output_tokens": 8192,
            }

            logger.info(
                f"[LLMè°ƒç”¨] å¼€å§‹è°ƒç”¨ Gemini API (temperature={generation_config['temperature']}, max_tokens={generation_config['max_output_tokens']})..."
            )

            # ä¸´æ—¶åˆ‡æ¢ç³»ç»Ÿæç¤ºè¯ä¸ºé»„é‡‘åˆ†æ Prompt
            original_system_prompt = self.SYSTEM_PROMPT
            self.SYSTEM_PROMPT = GOLD_SYSTEM_PROMPT

            # å¦‚æœä½¿ç”¨ Geminiï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹ä»¥åº”ç”¨æ–°çš„ç³»ç»Ÿæç¤ºè¯
            if self._model and not self._use_openai:
                try:
                    import google.generativeai as genai

                    self._model = genai.GenerativeModel(
                        model_name=self._current_model_name or "gemini-pro",
                        system_instruction=GOLD_SYSTEM_PROMPT,
                    )
                except Exception as e:
                    logger.warning(f"é‡æ–°åˆå§‹åŒ–æ¨¡å‹å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹: {e}")

            # ä½¿ç”¨å¸¦é‡è¯•çš„ API è°ƒç”¨
            start_time = time.time()
            response_text = self._call_api_with_retry(prompt, generation_config)
            elapsed = time.time() - start_time

            # æ¢å¤åŸå§‹ç³»ç»Ÿæç¤ºè¯
            self.SYSTEM_PROMPT = original_system_prompt
            if self._model and not self._use_openai:
                try:
                    import google.generativeai as genai

                    self._model = genai.GenerativeModel(
                        model_name=self._current_model_name or "gemini-pro",
                        system_instruction=original_system_prompt,
                    )
                except Exception as e:
                    logger.warning(f"æ¢å¤æ¨¡å‹å¤±è´¥: {e}")

            # è®°å½•å“åº”ä¿¡æ¯
            logger.info(f"[LLMè¿”å›] Gemini API å“åº”æˆåŠŸ, è€—æ—¶ {elapsed:.2f}s, å“åº”é•¿åº¦ {len(response_text)} å­—ç¬¦")

            # è®°å½•å“åº”é¢„è§ˆ
            response_preview = response_text[:300] + "..." if len(response_text) > 300 else response_text
            logger.info(f"[LLMè¿”å› é¢„è§ˆ]\n{response_preview}")
            logger.debug(f"=== Gemini å®Œæ•´å“åº” ({len(response_text)}å­—ç¬¦) ===\n{response_text}\n=== End Response ===")

            # è§£æå“åº”ï¼ˆä½¿ç”¨è§£æå™¨ï¼Œé»„é‡‘åˆ†æç»“æœæ ¼å¼ä¸è‚¡ç¥¨åˆ†æç›¸åŒï¼‰
            result = self._parser.parse(response_text, code, gold_name)
            result.raw_response = response_text
            result.search_performed = bool(news_context)

            logger.info(
                f"[LLMè§£æ] {gold_name}({code}) åˆ†æå®Œæˆ: {result.trend_prediction}, è¯„åˆ† {result.sentiment_score}"
            )

            return result

        except Exception as e:
            logger.error(f"AI åˆ†æé»„é‡‘ {gold_name}({code}) å¤±è´¥: {e}")
            return AnalysisResult(
                code=code,
                name=gold_name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)[:100]}",
                risk_warning="åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨åˆ†æ",
                success=False,
                error_message=str(e),
            )

    def _format_gold_prompt(self, context: Dict[str, Any], gold_name: str, news_context: Optional[str] = None) -> str:
        """
        æ ¼å¼åŒ–é»„é‡‘åˆ†ææç¤ºè¯

        åŒ…å«ï¼šæŠ€æœ¯æŒ‡æ ‡ã€ä»·æ ¼è¶‹åŠ¿ã€åŸºæœ¬é¢åˆ†æã€æ–°é—»

        Args:
            context: é»„é‡‘æ•°æ®ä¸Šä¸‹æ–‡
            gold_name: é»„é‡‘åç§°
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹
        """
        code = context.get("code", "AU")
        today = context.get("today", {})

        # ========== æ„å»ºé»„é‡‘åˆ†æè¾“å…¥ ==========
        prompt = f"""# é»„é‡‘äº¤æ˜“å†³ç­–åˆ†æè¯·æ±‚

## ğŸ“Š é»„é‡‘åŸºç¡€ä¿¡æ¯
| é¡¹ç›® | æ•°æ® |
|------|------|
| ä»£ç  | **{code}** |
| åç§° | **{gold_name}** |
| åˆ†ææ—¥æœŸ | {context.get('date', 'æœªçŸ¥')} |

---

## ğŸ“ˆ æŠ€æœ¯é¢æ•°æ®

### ä»Šæ—¥è¡Œæƒ…
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ”¶ç›˜ä»· | {today.get('close', 'N/A')} |
| å¼€ç›˜ä»· | {today.get('open', 'N/A')} |
| æœ€é«˜ä»· | {today.get('high', 'N/A')} |
| æœ€ä½ä»· | {today.get('low', 'N/A')} |
| æ¶¨è·Œå¹… | {today.get('pct_chg', 'N/A')}% |
| æˆäº¤é‡ | {self._format_volume(today.get('volume'))} |

### å‡çº¿ç³»ç»Ÿ
| å‡çº¿ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| MA5 | {today.get('ma5', 'N/A')} | çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA10 | {today.get('ma10', 'N/A')} | ä¸­çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA20 | {today.get('ma20', 'N/A')} | ä¸­æœŸè¶‹åŠ¿çº¿ |

### è¶‹åŠ¿åˆ†æ
"""

        # æ·»åŠ è¶‹åŠ¿åˆ†æç»“æœ
        if "trend_analysis" in context:
            trend = context["trend_analysis"]
            prompt += f"""
- **è¶‹åŠ¿çŠ¶æ€**: {trend.get('trend_status', 'æœªçŸ¥')}
- **å‡çº¿æ’åˆ—**: {trend.get('ma_alignment', 'æœªçŸ¥')}
- **è¶‹åŠ¿å¼ºåº¦**: {trend.get('trend_strength', 'æœªçŸ¥')}
- **ä¹°å…¥ä¿¡å·**: {trend.get('buy_signal', 'æœªçŸ¥')}
- **ä¿¡å·è¯„åˆ†**: {trend.get('signal_score', 'N/A')}
"""

        prompt += "\n---\n\n## ğŸ’° åŸºæœ¬é¢åˆ†æ\n\n"

        # æ·»åŠ æ–°é—»/èµ„è®¯ä¸Šä¸‹æ–‡
        if news_context:
            prompt += f"### å¸‚åœºèµ„è®¯\n{news_context}\n\n"
        else:
            prompt += "### å¸‚åœºèµ„è®¯\næš‚æ— æœ€æ–°èµ„è®¯ï¼Œè¯·åŸºäºæŠ€æœ¯é¢åˆ†æã€‚\n\n"

        prompt += """
---

## ğŸ“‹ åˆ†æè¦æ±‚

è¯·åŸºäºä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆå®Œæ•´çš„ã€é»„é‡‘äº¤æ˜“å†³ç­–ä»ªè¡¨ç›˜ã€‘JSON æ ¼å¼æŠ¥å‘Šã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. **æŠ€æœ¯é¢**ï¼šä»·æ ¼è¶‹åŠ¿ã€æ”¯æ’‘ä½/å‹åŠ›ä½ã€æˆäº¤é‡
2. **åŸºæœ¬é¢**ï¼šç¾å…ƒæŒ‡æ•°ã€é€šèƒ€æ•°æ®ã€ç¾è”å‚¨æ”¿ç­–ï¼ˆå¦‚èµ„è®¯ä¸­æœ‰æåŠï¼‰
3. **äº¤æ˜“å»ºè®®**ï¼šä¹°å…¥/å–å‡ºç‚¹ä½ã€æ­¢æŸä½ã€ç›®æ ‡ä½
4. **é£é™©æç¤º**ï¼šé»„é‡‘æ³¢åŠ¨è¾ƒå¤§ï¼ŒåŠ¡å¿…åŒ…å«æ˜ç¡®çš„é£é™©æç¤º

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µå®Œæ•´ã€‚
"""

        return prompt

    def batch_analyze(self, contexts: List[Dict[str, Any]], delay_between: float = 2.0) -> List[AnalysisResult]:
        """
        æ‰¹é‡åˆ†æå¤šåªè‚¡ç¥¨

        æ³¨æ„ï¼šä¸ºé¿å… API é€Ÿç‡é™åˆ¶ï¼Œæ¯æ¬¡åˆ†æä¹‹é—´ä¼šæœ‰å»¶è¿Ÿ

        Args:
            contexts: ä¸Šä¸‹æ–‡æ•°æ®åˆ—è¡¨
            delay_between: æ¯æ¬¡åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            AnalysisResult åˆ—è¡¨
        """
        results = []

        for i, context in enumerate(contexts):
            if i > 0:
                logger.debug(f"ç­‰å¾… {delay_between} ç§’åç»§ç»­...")
                time.sleep(delay_between)

            result = self.analyze(context)
            results.append(result)

        return results


# ä¾¿æ·å‡½æ•°
def get_analyzer() -> GeminiAnalyzer:
    """è·å– Gemini åˆ†æå™¨å®ä¾‹"""
    return GeminiAnalyzer()
