# -*- coding: utf-8 -*-
"""
Gemini AIåˆ†æå™¨å®ç°

ä»analyzer.pyè¿ç§»çš„GeminiAnalyzerç±»å®Œæ•´å®ç°
"""

import json
import logging

# å¯¼å…¥ä¾èµ–
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from tenacity import before_sleep_log, retry, retry_if_exception_type, stop_after_attempt, wait_exponential

project_root = Path(__file__).parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from common.config import get_config
from core.domain.analysis import AnalysisResult

from .parsers.dashboard_parser import DashboardParser
from .prompts.stock_analysis import SYSTEM_PROMPT

# è‚¡ç¥¨åç§°æ˜ å°„ï¼ˆå¸¸è§è‚¡ç¥¨ï¼‰
STOCK_NAME_MAP = {
    "600519": "è´µå·èŒ…å°",
    "000001": "å¹³å®‰é“¶è¡Œ",
    "300750": "å®å¾·æ—¶ä»£",
    "002594": "æ¯”äºšè¿ª",
    "600036": "æ‹›å•†é“¶è¡Œ",
    "601318": "ä¸­å›½å¹³å®‰",
    "000858": "äº”ç²®æ¶²",
    "600276": "æ’ç‘åŒ»è¯",
    "601012": "éš†åŸºç»¿èƒ½",
    "002475": "ç«‹è®¯ç²¾å¯†",
    "300059": "ä¸œæ–¹è´¢å¯Œ",
    "002415": "æµ·åº·å¨è§†",
    "600900": "é•¿æ±Ÿç”µåŠ›",
    "601166": "å…´ä¸šé“¶è¡Œ",
    "600028": "ä¸­å›½çŸ³åŒ–",
    "600674": "å·æŠ•èƒ½æº",
    "000919": "é‡‘é™µè¯ä¸š",
    "001206": "ä¾ä¾è‚¡ä»½",
    "002223": "é±¼è·ƒåŒ»ç–—",
}

logger = logging.getLogger(__name__)


class GeminiAnalyzer:
    """
    Gemini AI åˆ†æå™¨

    èŒè´£ï¼š
    1. è°ƒç”¨ Google Gemini API è¿›è¡Œè‚¡ç¥¨åˆ†æ
    2. ç»“åˆé¢„å…ˆæœç´¢çš„æ–°é—»å’ŒæŠ€æœ¯é¢æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Š
    3. è§£æ AI è¿”å›çš„ JSON æ ¼å¼ç»“æœ

    ä½¿ç”¨æ–¹å¼ï¼š
        analyzer = GeminiAnalyzer()
        result = analyzer.analyze(context, news_context)
    """

    # ä½¿ç”¨ä»promptsæ¨¡å—å¯¼å…¥çš„SYSTEM_PROMPT
    SYSTEM_PROMPT = SYSTEM_PROMPT

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– AI åˆ†æå™¨

        ä¼˜å…ˆçº§ï¼šGemini > OpenAI å…¼å®¹ API

        Args:
            api_key: Gemini API Keyï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        """
        config = get_config()
        self._api_key = api_key or config.gemini_api_key
        self._model = None
        self._current_model_name = None  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°
        self._using_fallback = False  # æ˜¯å¦æ­£åœ¨ä½¿ç”¨å¤‡é€‰æ¨¡å‹
        self._use_openai = False  # æ˜¯å¦ä½¿ç”¨ OpenAI å…¼å®¹ API
        self._openai_client = None  # OpenAI å®¢æˆ·ç«¯

        # åˆå§‹åŒ–è§£æå™¨
        self._parser = DashboardParser()

        # æ£€æŸ¥ Gemini API Key æ˜¯å¦æœ‰æ•ˆï¼ˆè¿‡æ»¤å ä½ç¬¦ï¼‰
        gemini_key_valid = self._api_key and not self._api_key.startswith("your_") and len(self._api_key) > 10

        # ä¼˜å…ˆå°è¯•åˆå§‹åŒ– Gemini
        if gemini_key_valid:
            try:
                self._init_model()
            except Exception as e:
                logger.warning(f"Gemini åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯• OpenAI å…¼å®¹ API")
                self._init_openai_fallback()
        else:
            # Gemini Key æœªé…ç½®ï¼Œå°è¯• OpenAI
            logger.info("Gemini API Key æœªé…ç½®ï¼Œå°è¯•ä½¿ç”¨ OpenAI å…¼å®¹ API")
            self._init_openai_fallback()

        # ä¸¤è€…éƒ½æœªé…ç½®
        if not self._model and not self._openai_client:
            logger.warning("æœªé…ç½®ä»»ä½• AI API Keyï¼ŒAI åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨")

    def _init_openai_fallback(self) -> None:
        """
        åˆå§‹åŒ– OpenAI å…¼å®¹ API ä½œä¸ºå¤‡é€‰

        æ”¯æŒæ‰€æœ‰ OpenAI æ ¼å¼çš„ APIï¼ŒåŒ…æ‹¬ï¼š
        - OpenAI å®˜æ–¹
        - DeepSeek
        - é€šä¹‰åƒé—®
        - Moonshot ç­‰
        """
        config = get_config()

        # æ£€æŸ¥ OpenAI API Key æ˜¯å¦æœ‰æ•ˆï¼ˆè¿‡æ»¤å ä½ç¬¦ï¼‰
        openai_key_valid = (
            config.openai_api_key and not config.openai_api_key.startswith("your_") and len(config.openai_api_key) > 10
        )

        if not openai_key_valid:
            logger.debug("OpenAI å…¼å®¹ API æœªé…ç½®æˆ–é…ç½®æ— æ•ˆ")
            return

        # åˆ†ç¦» import å’Œå®¢æˆ·ç«¯åˆ›å»ºï¼Œä»¥ä¾¿æä¾›æ›´å‡†ç¡®çš„é”™è¯¯ä¿¡æ¯
        try:
            from openai import OpenAI
        except ImportError:
            logger.error("æœªå®‰è£… openai åº“ï¼Œè¯·è¿è¡Œ: pip install openai")
            return

        try:
            # base_url å¯é€‰ï¼Œä¸å¡«åˆ™ä½¿ç”¨ OpenAI å®˜æ–¹é»˜è®¤åœ°å€
            client_kwargs = {"api_key": config.openai_api_key}
            if config.openai_base_url and config.openai_base_url.startswith("http"):
                client_kwargs["base_url"] = config.openai_base_url

            self._openai_client = OpenAI(**client_kwargs)
            self._current_model_name = config.openai_model
            self._use_openai = True
            logger.info(
                f"OpenAI å…¼å®¹ API åˆå§‹åŒ–æˆåŠŸ (base_url: {config.openai_base_url}, model: {config.openai_model})"
            )
        except ImportError as e:
            # ä¾èµ–ç¼ºå¤±ï¼ˆå¦‚ socksioï¼‰
            if "socksio" in str(e).lower() or "socks" in str(e).lower():
                logger.error(
                    f"OpenAI å®¢æˆ·ç«¯éœ€è¦ SOCKS ä»£ç†æ”¯æŒï¼Œè¯·è¿è¡Œ: pip install httpx[socks] æˆ– pip install socksio"
                )
            else:
                logger.error(f"OpenAI ä¾èµ–ç¼ºå¤±: {e}")
        except Exception as e:
            error_msg = str(e).lower()
            if "socks" in error_msg or "socksio" in error_msg or "proxy" in error_msg:
                logger.error(f"OpenAI ä»£ç†é…ç½®é”™è¯¯: {e}ï¼Œå¦‚ä½¿ç”¨ SOCKS ä»£ç†è¯·è¿è¡Œ: pip install httpx[socks]")
            else:
                logger.error(f"OpenAI å…¼å®¹ API åˆå§‹åŒ–å¤±è´¥: {e}")

    def _init_model(self) -> None:
        """
        åˆå§‹åŒ– Gemini æ¨¡å‹

        é…ç½®ï¼š
        - ä½¿ç”¨ gemini-3-flash-preview æˆ– gemini-2.5-flash æ¨¡å‹
        - ä¸å¯ç”¨ Google Searchï¼ˆä½¿ç”¨å¤–éƒ¨ Tavily/SerpAPI æœç´¢ï¼‰
        """
        try:
            import google.generativeai as genai

            # é…ç½® API Key
            genai.configure(api_key=self._api_key)

            # ä»é…ç½®è·å–æ¨¡å‹åç§°
            config = get_config()
            model_name = config.gemini_model
            fallback_model = config.gemini_model_fallback

            # ä¸å†ä½¿ç”¨ Google Search Groundingï¼ˆå·²çŸ¥æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
            # æ”¹ä¸ºä½¿ç”¨å¤–éƒ¨æœç´¢æœåŠ¡ï¼ˆTavily/SerpAPIï¼‰é¢„å…ˆè·å–æ–°é—»

            # å°è¯•åˆå§‹åŒ–ä¸»æ¨¡å‹
            try:
                self._model = genai.GenerativeModel(
                    model_name=model_name,
                    system_instruction=self.SYSTEM_PROMPT,
                )
                self._current_model_name = model_name
                self._using_fallback = False
                logger.info(f"Gemini æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {model_name})")
            except Exception as model_error:
                # å°è¯•å¤‡é€‰æ¨¡å‹
                logger.warning(f"ä¸»æ¨¡å‹ {model_name} åˆå§‹åŒ–å¤±è´¥: {model_error}ï¼Œå°è¯•å¤‡é€‰æ¨¡å‹ {fallback_model}")
                self._model = genai.GenerativeModel(
                    model_name=fallback_model,
                    system_instruction=self.SYSTEM_PROMPT,
                )
                self._current_model_name = fallback_model
                self._using_fallback = True
                logger.info(f"Gemini å¤‡é€‰æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {fallback_model})")

        except Exception as e:
            logger.error(f"Gemini æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self._model = None

    def _switch_to_fallback_model(self) -> bool:
        """
        åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹

        Returns:
            æ˜¯å¦æˆåŠŸåˆ‡æ¢
        """
        try:
            import google.generativeai as genai

            config = get_config()
            fallback_model = config.gemini_model_fallback

            logger.warning(f"[LLM] åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹: {fallback_model}")
            self._model = genai.GenerativeModel(
                model_name=fallback_model,
                system_instruction=self.SYSTEM_PROMPT,
            )
            self._current_model_name = fallback_model
            self._using_fallback = True
            logger.info(f"[LLM] å¤‡é€‰æ¨¡å‹ {fallback_model} åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"[LLM] åˆ‡æ¢å¤‡é€‰æ¨¡å‹å¤±è´¥: {e}")
            return False

    def is_available(self) -> bool:
        """æ£€æŸ¥åˆ†æå™¨æ˜¯å¦å¯ç”¨"""
        return self._model is not None or self._openai_client is not None

    def _call_openai_api(self, prompt: str, generation_config: dict) -> str:
        """
        è°ƒç”¨ OpenAI å…¼å®¹ API

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            å“åº”æ–‡æœ¬
        """
        config = get_config()
        max_retries = config.gemini_max_retries
        base_delay = config.gemini_retry_delay

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))
                    delay = min(delay, 60)
                    logger.info(f"[OpenAI] ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)

                response = self._openai_client.chat.completions.create(
                    model=self._current_model_name,
                    messages=[{"role": "system", "content": self.SYSTEM_PROMPT}, {"role": "user", "content": prompt}],
                    temperature=generation_config.get("temperature", 0.7),
                    max_tokens=generation_config.get("max_output_tokens", 8192),
                )

                if response and response.choices and response.choices[0].message.content:
                    return response.choices[0].message.content
                else:
                    raise ValueError("OpenAI API è¿”å›ç©ºå“åº”")

            except Exception as e:
                error_str = str(e)
                is_rate_limit = "429" in error_str or "rate" in error_str.lower() or "quota" in error_str.lower()

                if is_rate_limit:
                    logger.warning(f"[OpenAI] API é™æµï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")
                else:
                    logger.warning(f"[OpenAI] API è°ƒç”¨å¤±è´¥ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

                if attempt == max_retries - 1:
                    raise

        raise Exception("OpenAI API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")

    def _call_api_with_retry(self, prompt: str, generation_config: dict) -> str:
        """
        è°ƒç”¨ AI APIï¼Œå¸¦æœ‰é‡è¯•å’Œæ¨¡å‹åˆ‡æ¢æœºåˆ¶

        ä¼˜å…ˆçº§ï¼šGemini > Gemini å¤‡é€‰æ¨¡å‹ > OpenAI å…¼å®¹ API

        å¤„ç† 429 é™æµé”™è¯¯ï¼š
        1. å…ˆæŒ‡æ•°é€€é¿é‡è¯•
        2. å¤šæ¬¡å¤±è´¥ååˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹
        3. Gemini å®Œå…¨å¤±è´¥åå°è¯• OpenAI

        Args:
            prompt: æç¤ºè¯
            generation_config: ç”Ÿæˆé…ç½®

        Returns:
            å“åº”æ–‡æœ¬
        """
        # å¦‚æœå·²ç»åœ¨ä½¿ç”¨ OpenAI æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨ OpenAI
        if self._use_openai:
            return self._call_openai_api(prompt, generation_config)

        config = get_config()
        max_retries = config.gemini_max_retries
        base_delay = config.gemini_retry_delay

        last_error = None
        tried_fallback = getattr(self, "_using_fallback", False)

        for attempt in range(max_retries):
            try:
                # è¯·æ±‚å‰å¢åŠ å»¶æ—¶ï¼ˆé˜²æ­¢è¯·æ±‚è¿‡å¿«è§¦å‘é™æµï¼‰
                if attempt > 0:
                    delay = base_delay * (2 ** (attempt - 1))  # æŒ‡æ•°é€€é¿: 5, 10, 20, 40...
                    delay = min(delay, 60)  # æœ€å¤§60ç§’
                    logger.info(f"[Gemini] ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f} ç§’...")
                    time.sleep(delay)

                response = self._model.generate_content(
                    prompt, generation_config=generation_config, request_options={"timeout": 120}
                )

                if response and response.text:
                    return response.text
                else:
                    raise ValueError("Gemini è¿”å›ç©ºå“åº”")

            except Exception as e:
                last_error = e
                error_str = str(e)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ 429 é™æµé”™è¯¯
                is_rate_limit = "429" in error_str or "quota" in error_str.lower() or "rate" in error_str.lower()

                if is_rate_limit:
                    logger.warning(f"[Gemini] API é™æµ (429)ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

                    # å¦‚æœå·²ç»é‡è¯•äº†ä¸€åŠæ¬¡æ•°ä¸”è¿˜æ²¡åˆ‡æ¢è¿‡å¤‡é€‰æ¨¡å‹ï¼Œå°è¯•åˆ‡æ¢
                    if attempt >= max_retries // 2 and not tried_fallback:
                        if self._switch_to_fallback_model():
                            tried_fallback = True
                            logger.info("[Gemini] å·²åˆ‡æ¢åˆ°å¤‡é€‰æ¨¡å‹ï¼Œç»§ç»­é‡è¯•")
                        else:
                            logger.warning("[Gemini] åˆ‡æ¢å¤‡é€‰æ¨¡å‹å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹é‡è¯•")
                else:
                    # éé™æµé”™è¯¯ï¼Œè®°å½•å¹¶ç»§ç»­é‡è¯•
                    logger.warning(f"[Gemini] API è°ƒç”¨å¤±è´¥ï¼Œç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•: {error_str[:100]}")

        # Gemini æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯• OpenAI å…¼å®¹ API
        if self._openai_client:
            logger.warning("[Gemini] æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œåˆ‡æ¢åˆ° OpenAI å…¼å®¹ API")
            try:
                return self._call_openai_api(prompt, generation_config)
            except Exception as openai_error:
                logger.error(f"[OpenAI] å¤‡é€‰ API ä¹Ÿå¤±è´¥: {openai_error}")
                raise last_error or openai_error
        elif config.openai_api_key and config.openai_base_url:
            # å°è¯•æ‡’åŠ è½½åˆå§‹åŒ– OpenAI
            logger.warning("[Gemini] æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå°è¯•åˆå§‹åŒ– OpenAI å…¼å®¹ API")
            self._init_openai_fallback()
            if self._openai_client:
                try:
                    return self._call_openai_api(prompt, generation_config)
                except Exception as openai_error:
                    logger.error(f"[OpenAI] å¤‡é€‰ API ä¹Ÿå¤±è´¥: {openai_error}")
                    raise last_error or openai_error

        # æ‰€æœ‰æ–¹å¼éƒ½å¤±è´¥
        raise last_error or Exception("æ‰€æœ‰ AI API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")

    def analyze(self, context: Dict[str, Any], news_context: Optional[str] = None) -> AnalysisResult:
        """
        åˆ†æå•åªè‚¡ç¥¨

        æµç¨‹ï¼š
        1. æ ¼å¼åŒ–è¾“å…¥æ•°æ®ï¼ˆæŠ€æœ¯é¢ + æ–°é—»ï¼‰
        2. è°ƒç”¨ Gemini APIï¼ˆå¸¦é‡è¯•å’Œæ¨¡å‹åˆ‡æ¢ï¼‰
        3. è§£æ JSON å“åº”
        4. è¿”å›ç»“æ„åŒ–ç»“æœ

        Args:
            context: ä» storage.get_analysis_context() è·å–çš„ä¸Šä¸‹æ–‡æ•°æ®
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹ï¼ˆå¯é€‰ï¼‰

        Returns:
            AnalysisResult å¯¹è±¡
        """
        code = context.get("code", "Unknown")
        config = get_config()

        # è¯·æ±‚å‰å¢åŠ å»¶æ—¶ï¼ˆé˜²æ­¢è¿ç»­è¯·æ±‚è§¦å‘é™æµï¼‰
        request_delay = config.gemini_request_delay
        if request_delay > 0:
            logger.debug(f"[LLM] è¯·æ±‚å‰ç­‰å¾… {request_delay:.1f} ç§’...")
            time.sleep(request_delay)

        # ä¼˜å…ˆä»ä¸Šä¸‹æ–‡è·å–è‚¡ç¥¨åç§°ï¼ˆç”± main.py ä¼ å…¥ï¼‰
        name = context.get("stock_name")
        if not name or name.startswith("è‚¡ç¥¨"):
            # å¤‡é€‰ï¼šä» realtime ä¸­è·å–
            if "realtime" in context and context["realtime"].get("name"):
                name = context["realtime"]["name"]
            else:
                # æœ€åä»æ˜ å°„è¡¨è·å–
                name = STOCK_NAME_MAP.get(code, f"è‚¡ç¥¨{code}")

        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤ç»“æœ
        if not self.is_available():
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary="AI åˆ†æåŠŸèƒ½æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰",
                risk_warning="è¯·é…ç½® Gemini API Key åé‡è¯•",
                success=False,
                error_message="Gemini API Key æœªé…ç½®",
            )

        try:
            # æ ¼å¼åŒ–è¾“å…¥ï¼ˆåŒ…å«æŠ€æœ¯é¢æ•°æ®å’Œæ–°é—»ï¼‰
            prompt = self._format_prompt(context, name, news_context)

            # è·å–æ¨¡å‹åç§°
            model_name = getattr(self, "_current_model_name", None)
            if not model_name:
                model_name = getattr(self._model, "_model_name", "unknown")
                if hasattr(self._model, "model_name"):
                    model_name = self._model.model_name

            logger.info(f"========== AI åˆ†æ {name}({code}) ==========")
            logger.info(f"[LLMé…ç½®] æ¨¡å‹: {model_name}")
            logger.info(f"[LLMé…ç½®] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"[LLMé…ç½®] æ˜¯å¦åŒ…å«æ–°é—»: {'æ˜¯' if news_context else 'å¦'}")

            # è®°å½•å®Œæ•´ prompt åˆ°æ—¥å¿—ï¼ˆINFOçº§åˆ«è®°å½•æ‘˜è¦ï¼ŒDEBUGè®°å½•å®Œæ•´ï¼‰
            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
            logger.info(f"[LLM Prompt é¢„è§ˆ]\n{prompt_preview}")
            logger.debug(f"=== å®Œæ•´ Prompt ({len(prompt)}å­—ç¬¦) ===\n{prompt}\n=== End Prompt ===")

            # è®¾ç½®ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": 0.7,
                "max_output_tokens": 8192,
            }

            logger.info(
                f"[LLMè°ƒç”¨] å¼€å§‹è°ƒç”¨ Gemini API (temperature={generation_config['temperature']}, max_tokens={generation_config['max_output_tokens']})..."
            )

            # ä½¿ç”¨å¸¦é‡è¯•çš„ API è°ƒç”¨
            start_time = time.time()
            response_text = self._call_api_with_retry(prompt, generation_config)
            elapsed = time.time() - start_time

            # è®°å½•å“åº”ä¿¡æ¯
            logger.info(f"[LLMè¿”å›] Gemini API å“åº”æˆåŠŸ, è€—æ—¶ {elapsed:.2f}s, å“åº”é•¿åº¦ {len(response_text)} å­—ç¬¦")

            # è®°å½•å“åº”é¢„è§ˆï¼ˆINFOçº§åˆ«ï¼‰å’Œå®Œæ•´å“åº”ï¼ˆDEBUGçº§åˆ«ï¼‰
            response_preview = response_text[:300] + "..." if len(response_text) > 300 else response_text
            logger.info(f"[LLMè¿”å› é¢„è§ˆ]\n{response_preview}")
            logger.debug(f"=== Gemini å®Œæ•´å“åº” ({len(response_text)}å­—ç¬¦) ===\n{response_text}\n=== End Response ===")

            # è§£æå“åº”ï¼ˆä½¿ç”¨è§£æå™¨ï¼‰
            result = self._parser.parse(response_text, code, name)
            result.raw_response = response_text
            result.search_performed = bool(news_context)

            logger.info(f"[LLMè§£æ] {name}({code}) åˆ†æå®Œæˆ: {result.trend_prediction}, è¯„åˆ† {result.sentiment_score}")

            return result

        except Exception as e:
            logger.error(f"AI åˆ†æ {name}({code}) å¤±è´¥: {e}")
            return AnalysisResult(
                code=code,
                name=name,
                sentiment_score=50,
                trend_prediction="éœ‡è¡",
                operation_advice="æŒæœ‰",
                confidence_level="ä½",
                analysis_summary=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)[:100]}",
                risk_warning="åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•æˆ–æ‰‹åŠ¨åˆ†æ",
                success=False,
                error_message=str(e),
            )

    def _format_prompt(self, context: Dict[str, Any], name: str, news_context: Optional[str] = None) -> str:
        """
        æ ¼å¼åŒ–åˆ†ææç¤ºè¯ï¼ˆå†³ç­–ä»ªè¡¨ç›˜ v2.0ï¼‰

        åŒ…å«ï¼šæŠ€æœ¯æŒ‡æ ‡ã€å®æ—¶è¡Œæƒ…ï¼ˆé‡æ¯”/æ¢æ‰‹ç‡ï¼‰ã€ç­¹ç åˆ†å¸ƒã€è¶‹åŠ¿åˆ†æã€æ–°é—»

        Args:
            context: æŠ€æœ¯é¢æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å¢å¼ºæ•°æ®ï¼‰
            name: è‚¡ç¥¨åç§°ï¼ˆé»˜è®¤å€¼ï¼Œå¯èƒ½è¢«ä¸Šä¸‹æ–‡è¦†ç›–ï¼‰
            news_context: é¢„å…ˆæœç´¢çš„æ–°é—»å†…å®¹
        """
        code = context.get("code", "Unknown")

        # ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„è‚¡ç¥¨åç§°ï¼ˆä» realtime_quote è·å–ï¼‰
        stock_name = context.get("stock_name", name)
        if not stock_name or stock_name == f"è‚¡ç¥¨{code}":
            stock_name = STOCK_NAME_MAP.get(code, f"è‚¡ç¥¨{code}")

        today = context.get("today", {})

        # ========== æ„å»ºå†³ç­–ä»ªè¡¨ç›˜æ ¼å¼çš„è¾“å…¥ ==========
        prompt = f"""# å†³ç­–ä»ªè¡¨ç›˜åˆ†æè¯·æ±‚

## ğŸ“Š è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
| é¡¹ç›® | æ•°æ® |
|------|------|
| è‚¡ç¥¨ä»£ç  | **{code}** |
| è‚¡ç¥¨åç§° | **{stock_name}** |
| åˆ†ææ—¥æœŸ | {context.get('date', 'æœªçŸ¥')} |

---

## ğŸ“ˆ æŠ€æœ¯é¢æ•°æ®

### ä»Šæ—¥è¡Œæƒ…
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ”¶ç›˜ä»· | {today.get('close', 'N/A')} å…ƒ |
| å¼€ç›˜ä»· | {today.get('open', 'N/A')} å…ƒ |
| æœ€é«˜ä»· | {today.get('high', 'N/A')} å…ƒ |
| æœ€ä½ä»· | {today.get('low', 'N/A')} å…ƒ |
| æ¶¨è·Œå¹… | {today.get('pct_chg', 'N/A')}% |
| æˆäº¤é‡ | {self._format_volume(today.get('volume'))} |
| æˆäº¤é¢ | {self._format_amount(today.get('amount'))} |

### å‡çº¿ç³»ç»Ÿï¼ˆå…³é”®åˆ¤æ–­æŒ‡æ ‡ï¼‰
| å‡çº¿ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| MA5 | {today.get('ma5', 'N/A')} | çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA10 | {today.get('ma10', 'N/A')} | ä¸­çŸ­æœŸè¶‹åŠ¿çº¿ |
| MA20 | {today.get('ma20', 'N/A')} | ä¸­æœŸè¶‹åŠ¿çº¿ |
| å‡çº¿å½¢æ€ | {context.get('ma_status', 'æœªçŸ¥')} | å¤šå¤´/ç©ºå¤´/ç¼ ç»• |
"""

        # æ·»åŠ å®æ—¶è¡Œæƒ…æ•°æ®ï¼ˆé‡æ¯”ã€æ¢æ‰‹ç‡ç­‰ï¼‰
        if "realtime" in context:
            rt = context["realtime"]
            prompt += f"""
### å®æ—¶è¡Œæƒ…å¢å¼ºæ•°æ®
| æŒ‡æ ‡ | æ•°å€¼ | è§£è¯» |
|------|------|------|
| å½“å‰ä»·æ ¼ | {rt.get('price', 'N/A')} å…ƒ | |
| **é‡æ¯”** | **{rt.get('volume_ratio', 'N/A')}** | {rt.get('volume_ratio_desc', '')} |
| **æ¢æ‰‹ç‡** | **{rt.get('turnover_rate', 'N/A')}%** | |
| å¸‚ç›ˆç‡(åŠ¨æ€) | {rt.get('pe_ratio', 'N/A')} | |
| å¸‚å‡€ç‡ | {rt.get('pb_ratio', 'N/A')} | |
| æ€»å¸‚å€¼ | {self._format_amount(rt.get('total_mv'))} | |
| æµé€šå¸‚å€¼ | {self._format_amount(rt.get('circ_mv'))} | |
| 60æ—¥æ¶¨è·Œå¹… | {rt.get('change_60d', 'N/A')}% | ä¸­æœŸè¡¨ç° |
"""

        # æ·»åŠ ç­¹ç åˆ†å¸ƒæ•°æ®
        if "chip" in context:
            chip = context["chip"]
            profit_ratio = chip.get("profit_ratio", 0)
            prompt += f"""
### ç­¹ç åˆ†å¸ƒæ•°æ®ï¼ˆæ•ˆç‡æŒ‡æ ‡ï¼‰
| æŒ‡æ ‡ | æ•°å€¼ | å¥åº·æ ‡å‡† |
|------|------|----------|
| **è·åˆ©æ¯”ä¾‹** | **{profit_ratio:.1%}** | 70-90%æ—¶è­¦æƒ• |
| å¹³å‡æˆæœ¬ | {chip.get('avg_cost', 'N/A')} å…ƒ | ç°ä»·åº”é«˜äº5-15% |
| 90%ç­¹ç é›†ä¸­åº¦ | {chip.get('concentration_90', 0):.2%} | <15%ä¸ºé›†ä¸­ |
| 70%ç­¹ç é›†ä¸­åº¦ | {chip.get('concentration_70', 0):.2%} | |
| ç­¹ç çŠ¶æ€ | {chip.get('chip_status', 'æœªçŸ¥')} | |
"""

        # æ·»åŠ è¶‹åŠ¿åˆ†æç»“æœï¼ˆåŸºäºäº¤æ˜“ç†å¿µçš„é¢„åˆ¤ï¼‰
        if "trend_analysis" in context:
            trend = context["trend_analysis"]
            bias_warning = "ğŸš¨ è¶…è¿‡5%ï¼Œä¸¥ç¦è¿½é«˜ï¼" if trend.get("bias_ma5", 0) > 5 else "âœ… å®‰å…¨èŒƒå›´"
            prompt += f"""
### è¶‹åŠ¿åˆ†æé¢„åˆ¤ï¼ˆåŸºäºäº¤æ˜“ç†å¿µï¼‰
| æŒ‡æ ‡ | æ•°å€¼ | åˆ¤å®š |
|------|------|------|
| è¶‹åŠ¿çŠ¶æ€ | {trend.get('trend_status', 'æœªçŸ¥')} | |
| å‡çº¿æ’åˆ— | {trend.get('ma_alignment', 'æœªçŸ¥')} | MA5>MA10>MA20ä¸ºå¤šå¤´ |
| è¶‹åŠ¿å¼ºåº¦ | {trend.get('trend_strength', 0)}/100 | |
| **ä¹–ç¦»ç‡(MA5)** | **{trend.get('bias_ma5', 0):+.2f}%** | {bias_warning} |
| ä¹–ç¦»ç‡(MA10) | {trend.get('bias_ma10', 0):+.2f}% | |
| é‡èƒ½çŠ¶æ€ | {trend.get('volume_status', 'æœªçŸ¥')} | {trend.get('volume_trend', '')} |
| ç³»ç»Ÿä¿¡å· | {trend.get('buy_signal', 'æœªçŸ¥')} | |
| ç³»ç»Ÿè¯„åˆ† | {trend.get('signal_score', 0)}/100 | |

#### ç³»ç»Ÿåˆ†æç†ç”±
**ä¹°å…¥ç†ç”±**ï¼š
{chr(10).join('- ' + r for r in trend.get('signal_reasons', ['æ— '])) if trend.get('signal_reasons') else '- æ— '}

**é£é™©å› ç´ **ï¼š
{chr(10).join('- ' + r for r in trend.get('risk_factors', ['æ— '])) if trend.get('risk_factors') else '- æ— '}
"""

        # æ·»åŠ æ˜¨æ—¥å¯¹æ¯”æ•°æ®
        if "yesterday" in context:
            volume_change = context.get("volume_change_ratio", "N/A")
            prompt += f"""
### é‡ä»·å˜åŒ–
- æˆäº¤é‡è¾ƒæ˜¨æ—¥å˜åŒ–ï¼š{volume_change}å€
- ä»·æ ¼è¾ƒæ˜¨æ—¥å˜åŒ–ï¼š{context.get('price_change_ratio', 'N/A')}%
"""

        # æ·»åŠ æ–°é—»æœç´¢ç»“æœï¼ˆé‡ç‚¹åŒºåŸŸï¼‰
        prompt += """
---

## ğŸ“° èˆ†æƒ…æƒ…æŠ¥
"""
        if news_context:
            prompt += f"""
ä»¥ä¸‹æ˜¯ **{stock_name}({code})** è¿‘7æ—¥çš„æ–°é—»æœç´¢ç»“æœï¼Œè¯·é‡ç‚¹æå–ï¼š
1. ğŸš¨ **é£é™©è­¦æŠ¥**ï¼šå‡æŒã€å¤„ç½šã€åˆ©ç©º
2. ğŸ¯ **åˆ©å¥½å‚¬åŒ–**ï¼šä¸šç»©ã€åˆåŒã€æ”¿ç­–
3. ğŸ“Š **ä¸šç»©é¢„æœŸ**ï¼šå¹´æŠ¥é¢„å‘Šã€ä¸šç»©å¿«æŠ¥

```
{news_context}
```
"""
        else:
            prompt += """
æœªæœç´¢åˆ°è¯¥è‚¡ç¥¨è¿‘æœŸçš„ç›¸å…³æ–°é—»ã€‚è¯·ä¸»è¦ä¾æ®æŠ€æœ¯é¢æ•°æ®è¿›è¡Œåˆ†æã€‚
"""

        # æ˜ç¡®çš„è¾“å‡ºè¦æ±‚
        prompt += f"""
---

## âœ… åˆ†æä»»åŠ¡

è¯·ä¸º **{stock_name}({code})** ç”Ÿæˆã€å†³ç­–ä»ªè¡¨ç›˜ã€‘ï¼Œä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºã€‚

### é‡ç‚¹å…³æ³¨ï¼ˆå¿…é¡»æ˜ç¡®å›ç­”ï¼‰ï¼š
1. â“ æ˜¯å¦æ»¡è¶³ MA5>MA10>MA20 å¤šå¤´æ’åˆ—ï¼Ÿ
2. â“ å½“å‰ä¹–ç¦»ç‡æ˜¯å¦åœ¨å®‰å…¨èŒƒå›´å†…ï¼ˆ<5%ï¼‰ï¼Ÿâ€”â€” è¶…è¿‡5%å¿…é¡»æ ‡æ³¨"ä¸¥ç¦è¿½é«˜"
3. â“ é‡èƒ½æ˜¯å¦é…åˆï¼ˆç¼©é‡å›è°ƒ/æ”¾é‡çªç ´ï¼‰ï¼Ÿ
4. â“ ç­¹ç ç»“æ„æ˜¯å¦å¥åº·ï¼Ÿ
5. â“ æ¶ˆæ¯é¢æœ‰æ— é‡å¤§åˆ©ç©ºï¼Ÿï¼ˆå‡æŒã€å¤„ç½šã€ä¸šç»©å˜è„¸ç­‰ï¼‰

### å†³ç­–ä»ªè¡¨ç›˜è¦æ±‚ï¼š
- **æ ¸å¿ƒç»“è®º**ï¼šä¸€å¥è¯è¯´æ¸…è¯¥ä¹°/è¯¥å–/è¯¥ç­‰
- **æŒä»“åˆ†ç±»å»ºè®®**ï¼šç©ºä»“è€…æ€ä¹ˆåš vs æŒä»“è€…æ€ä¹ˆåš
- **å…·ä½“ç‹™å‡»ç‚¹ä½**ï¼šä¹°å…¥ä»·ã€æ­¢æŸä»·ã€ç›®æ ‡ä»·ï¼ˆç²¾ç¡®åˆ°åˆ†ï¼‰
- **æ£€æŸ¥æ¸…å•**ï¼šæ¯é¡¹ç”¨ âœ…/âš ï¸/âŒ æ ‡è®°

è¯·è¾“å‡ºå®Œæ•´çš„ JSON æ ¼å¼å†³ç­–ä»ªè¡¨ç›˜ã€‚"""

        return prompt

    def _format_volume(self, volume: Optional[float]) -> str:
        """æ ¼å¼åŒ–æˆäº¤é‡æ˜¾ç¤º"""
        if volume is None:
            return "N/A"
        if volume >= 1e8:
            return f"{volume / 1e8:.2f} äº¿è‚¡"
        elif volume >= 1e4:
            return f"{volume / 1e4:.2f} ä¸‡è‚¡"
        else:
            return f"{volume:.0f} è‚¡"

    def _format_amount(self, amount: Optional[float]) -> str:
        """æ ¼å¼åŒ–æˆäº¤é¢æ˜¾ç¤º"""
        if amount is None:
            return "N/A"
        if amount >= 1e8:
            return f"{amount / 1e8:.2f} äº¿å…ƒ"
        elif amount >= 1e4:
            return f"{amount / 1e4:.2f} ä¸‡å…ƒ"
        else:
            return f"{amount:.0f} å…ƒ"

    def batch_analyze(self, contexts: List[Dict[str, Any]], delay_between: float = 2.0) -> List[AnalysisResult]:
        """
        æ‰¹é‡åˆ†æå¤šåªè‚¡ç¥¨

        æ³¨æ„ï¼šä¸ºé¿å… API é€Ÿç‡é™åˆ¶ï¼Œæ¯æ¬¡åˆ†æä¹‹é—´ä¼šæœ‰å»¶è¿Ÿ

        Args:
            contexts: ä¸Šä¸‹æ–‡æ•°æ®åˆ—è¡¨
            delay_between: æ¯æ¬¡åˆ†æä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            AnalysisResult åˆ—è¡¨
        """
        results = []

        for i, context in enumerate(contexts):
            if i > 0:
                logger.debug(f"ç­‰å¾… {delay_between} ç§’åç»§ç»­...")
                time.sleep(delay_between)

            result = self.analyze(context)
            results.append(result)

        return results


# ä¾¿æ·å‡½æ•°
def get_analyzer() -> GeminiAnalyzer:
    """è·å– Gemini åˆ†æå™¨å®ä¾‹"""
    return GeminiAnalyzer()
